# AI-Powered Text Analysis Platform - AI Instructions

## Project Overview
This is an AI-driven development project building a comprehensive web application for analyzing user queries, context, and AI responses using NLTK and LLM APIs. The platform combines proven word cloud functionality with a robust FastAPI backend for large-scale data processing.

## Development Guidelines

### Git Commit Standards ‚ö†Ô∏è CRITICAL
**IMPORTANT**: Use SHORT commit messages only. Long commit messages break deployment pipelines.

**Format Rules:**
- Maximum 50 characters
- Start with action verb (Add, Fix, Update, Remove)
- No multi-line descriptions
- No excessive emojis

**Examples:**
- ‚úÖ "Add analytics dashboard"
- ‚úÖ "Fix CORS configuration" 
- ‚úÖ "Update database models"
- ‚ùå "üéØ NEW: Comprehensive Analytics Dashboard with multiple features..."

## Core Architecture

### Frontend Stack (Next.js 14 on Vercel)
- **Framework**: Next.js 14 with TypeScript
- **Styling**: Tailwind CSS with custom components
- **Data Tables**: AG Grid React for interactive data visualization
- **State Management**: React Context API or Zustand
- **Authentication**: Clerk integration
- **Real-time**: WebSocket client for job status updates
- **Visualization**: Custom CSS-based word cloud + Recharts

### Backend Stack (Unified FastAPI on Railway)
- **Framework**: FastAPI with Python 3.11+ (Consolidated Architecture)
- **Database**: PostgreSQL with SQLAlchemy ORM + Service Layer
- **Job Queue**: Celery with Redis broker  
- **File Storage**: Local filesystem with robust validation (AWS S3 ready)
- **Authentication**: JWT tokens + Clerk integration (temporarily disabled)
- **WebSocket**: FastAPI WebSocket for real-time updates
- **Text Analysis**: NLTK + OpenAI LLM integration
- **Architecture**: Unified server with separated service and API layers

## Service Map

### Core Services (Unified Architecture)
1. **Dataset Service Layer** (`backend/app/services/dataset_service.py`)
   - **ROBUST**: Comprehensive file validation and error handling
   - **SECURE**: Multi-encoding support, file type validation, size limits
   - **ATOMIC**: Database transactions with automatic rollback
   - **SCALABLE**: Pagination, efficient queries, background processing
   - Upload, preview, and manage query-response datasets
   - Real-time processing status updates

2. **Analysis Service Layer** (`backend/app/services/analysis_service.py`)
   - **NLTK-Powered**: POS tagging for precise word categorization
   - **CACHED**: Intelligent caching with force-regeneration options
   - **MODES**: True analysis mode differentiation (verbs, nouns, adjectives, emotions)
   - Sentiment analysis with enhanced word filtering
   - Word frequency generation with database persistence

3. **NEW: Schema Detection Service** (`backend/app/services/schema_detection_service.py`)
   - **JSON/CSV Analysis**: Automatic schema detection for any data structure
   - **AI Field Classification**: ML-powered field role detection (primary text, category, etc.)
   - **Type Detection**: Intelligent data type inference (text, number, date, email, etc.)
   - **Confidence Scoring**: Machine learning confidence metrics for all detections
   - **Flexible Mapping**: Support for nested JSON objects and complex data structures

4. **Dataset API Endpoints** (`backend/app/api/datasets.py`)
   - **RESTful**: Complete CRUD operations with proper HTTP methods
   - **VALIDATED**: Pydantic models and comprehensive input validation  
   - **DOCUMENTED**: OpenAPI/Swagger documentation with examples
   - **ROBUST**: Proper error handling and status codes
   - Upload, list, retrieve, delete, and reprocess datasets

5. **NEW: Schema API Endpoints** (`backend/app/api/schema.py`)
   - **Schema Detection**: `/api/schema/detect` - AI-powered schema inference from uploaded files
   - **Field Mapping**: `/api/schema/mapping` - Save user-defined field roles and analysis config
   - **AI Suggestions**: `/api/schema/suggestions/{dataset_id}` - Get intelligent field mapping suggestions
   - **Analysis Preview**: `/api/schema/preview` - Preview what will be analyzed with current config
   - **Schema Retrieval**: `/api/schema/{dataset_id}` - Get detected schema for a dataset

6. **Word Cloud System** (`frontend/components/wordcloud/WordCloudVisualization.tsx`)
   - Multi-mode support (entities, topics, sentiment, themes)
   - Interactive exploration with drill-down capability  
   - Real-time filtering and comparative analysis
   - Export functionality (PNG, SVG, PDF)

4. **Dataset Table View** (`frontend/components/datasets/DatasetTableView.tsx`)
   - AG Grid React integration for high-performance data tables
   - Interactive sorting, filtering, and search functionality
   - Auto-sizing columns with text wrapping for long content
   - CSV export functionality with filtered data
   - Pagination support for large datasets

5. **Real-time Processing** (`backend/app/websocket/manager.py`)
   - WebSocket connection management
   - Background job status updates
   - Progress tracking for large dataset processing

6. **Advanced Analytics** (`backend/app/api/analytics.py`)
   - Sentiment trend analysis over time
   - Topic evolution tracking
   - Entity relationship networks
   - Conversation quality metrics

7. **Export & Reporting** (`backend/app/reporting/`)
   - Executive summary generation
   - Multi-format exports (PDF, Excel, PowerPoint)
   - Scheduled report generation
   - Custom report templates

## Database Schema

### Core Tables
- **users**: Clerk integration for user management
- **datasets**: Enhanced with processing status and metrics
- **questions**: Query-response pairs with NLTK analysis results
- **nltk_analysis**: Detailed analysis results (entities, topics, keywords)
- **word_frequencies**: Word cloud data with sentiment associations
- **analysis_jobs**: Background processing job tracking
- **llm_analysis_cache**: Cached LLM responses for efficiency
- **org_usage_analytics**: Organization-level usage metrics

## Key Features

### 1. Enhanced Data Processing
- Bulk upload of query-response CSV files
- Real-time processing with progress updates
- NLTK preprocessing and analysis pipeline
- Quality validation and error handling

### 2. Advanced Text Analysis
- **Sentiment Analysis**: Multi-model approach with confidence scoring
- **Topic Modeling**: LDA with coherence metrics and evolution tracking
- **Entity Extraction**: Named entity recognition and relationship mapping
- **Question Classification**: Type classification and complexity scoring
- **Response Quality**: Relevance, completeness, and alignment analysis

### 3. Interactive Visualizations
- **Enhanced Word Clouds**: Multi-mode analysis with interactive features
- **Sentiment Dashboards**: Time-series analysis with trend identification
- **Topic Exploration**: Interactive topic modeling with drill-down
- **Entity Networks**: Relationship visualization between extracted entities

### 4. Real-time Processing
- WebSocket-based progress updates
- Background job queue with Celery
- Status tracking and error handling
- Concurrent processing optimization

## File Structure

```
WordCloud/
‚îú‚îÄ‚îÄ AI_Instructions.md (this file)
‚îú‚îÄ‚îÄ AI_System_Prompt.md
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard/ (analytics components)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wordcloud/ (visualization components)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/ (data management with AG Grid tables)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/ (authentication)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common/ (shared components)
‚îÇ   ‚îú‚îÄ‚îÄ lib/ (utilities and API clients)
‚îÇ   ‚îú‚îÄ‚îÄ pages/ (Next.js pages)
‚îÇ   ‚îî‚îÄ‚îÄ styles/ (Tailwind CSS)
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/ (UNIFIED FASTAPI APPLICATION)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py (Main FastAPI application with proper architecture)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/ (SQLAlchemy database models with relationships)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/ (REST API endpoints - FULLY IMPLEMENTED)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/ (Business logic and data access layer - NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis/ (NLTK processing engines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/ (Celery background tasks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ websocket/ (real-time connections)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reporting/ (export functionality)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ core/ (configuration, database, logging)
‚îÇ   ‚îú‚îÄ‚îÄ unified_production_server.py (Production server - REPLACES old servers)
‚îÇ   ‚îú‚îÄ‚îÄ test_unified_system.py (End-to-end testing script)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ railway.toml (Updated for unified server)
‚îÇ   ‚îî‚îÄ‚îÄ alembic/ (database migrations)
‚îî‚îÄ‚îÄ docs/ (additional documentation)
```

## Development Workflow

### Phase 1: Infrastructure Setup ‚úÖ (Current)
- Core documentation and project structure
- Database models and migrations
- Authentication integration
- Basic API endpoints

### Phase 2: NLTK Analysis Engine ‚úÖ **COMPLETED**
- ‚úÖ Sentiment analysis pipeline (VADER + TextBlob + custom models)
- ‚úÖ Topic modeling implementation (LDA with coherence scoring)
- ‚úÖ Entity extraction setup (spaCy integration with fallback)
- ‚úÖ Question classification system (intent analysis and complexity scoring)
- ‚úÖ Keyword extraction (TF-IDF, YAKE, TextRank methods)
- ‚úÖ Readability analysis and text similarity metrics

### Phase 3: Word Cloud Enhancement ‚úÖ **COMPLETED**
- ‚úÖ Multi-mode visualization (entities, topics, sentiment, themes)
- ‚úÖ Interactive features (hover, click, zoom, filter)
- ‚úÖ Real-time updates (dynamic column filtering)
- ‚úÖ Export functionality (PNG, SVG support)
- ‚úÖ Advanced word filtering with user-specified exclusions
- ‚úÖ Column selection and data filtering capabilities

### Phase 4: LLM Integration
- OpenAI API integration
- Response quality analysis
- Business insights generation
- Intelligent caching

### Phase 5: Dashboard & Reporting
- Analytics dashboard
- Multi-format exports
- Report templates
- Scheduled reporting

### Phase 6: Frontend Integration
- Component integration
- Responsive design
- Error handling
- Performance optimization

### Phase 7: Testing & Deployment
- Comprehensive testing
- Security audit
- Performance optimization
- Production deployment

## Deployment Configuration

### Environment Variables
- Database: PostgreSQL connection
- Redis: Job queue and caching
- OpenAI: LLM API integration
- Clerk: Authentication services
- AWS: S3 file storage
- Security: JWT and rate limiting

### Infrastructure
- **Frontend**: Vercel deployment
- **Backend**: Railway hosting
- **Database**: Railway PostgreSQL
- **Cache/Queue**: Railway Redis
- **Storage**: AWS S3 with CloudFront

### Railway Configuration (WORKING SETUP ‚úÖ)
```toml
[build]
builder = "dockerfile"

[deploy]
startCommand = "python api_server_db.py"
healthcheckPath = "/health"
healthcheckTimeout = 300
restartPolicyType = "always"

[env]
APP_ENV = { default = "production" }
DEBUG = { default = "false" }
PYTHONPATH = "/app/backend"
CORS_ORIGINS = { default = "https://ai-text-analysis-platform.vercel.app,..." }
UPLOAD_DIR = { default = "/app/backend/uploads" }
MAX_FILE_SIZE = { default = "104857600" }
```

**Critical Notes:**
- Must use `dockerfile` builder (nixpacks causes path issues)
- `PYTHONPATH="/app/backend"` is essential for Python imports
- `startCommand` runs without `backend/` prefix due to PYTHONPATH setting
- Health check at `/health` matches FastAPI endpoint

## AI Development Guidelines

### When Working on This Project:
1. **Always** update this AI_Instructions.md when adding new services or features
2. **Always** create/update README.md files in directories when adding new components
3. **Follow** the established architecture patterns and file structure
4. **Maintain** the service map and database schema documentation
5. **Document** all API endpoints and their purposes
6. **Update** the development phase status as features are completed

### Code Standards:
- Use TypeScript for all frontend code
- Use Pydantic models for all API schemas
- Include comprehensive error handling
- Add proper logging for debugging
- Follow RESTful API design principles
- Implement proper security measures

### Testing Requirements:
- Unit tests for all analysis functions
- Integration tests for API endpoints
- E2E tests for critical user flows
- Load testing for large datasets
- Security testing for authentication

## Current Status (v3.0.0 - UNIFIED ARCHITECTURE)

### ‚úÖ **MAJOR CONSOLIDATION COMPLETED** (January 2025)
- **üÜï Server Architecture Unified**: All duplicate servers consolidated into single FastAPI application
- **üÜï Service Layer Implementation**: Robust business logic separation with `DatasetService` and `AnalysisService`
- **üÜï Enhanced Data Upload**: Comprehensive file validation, atomic transactions, automatic rollback
- **üÜï Production Server Updated**: `unified_production_server.py` replaces fragmented server implementations
- **üÜï End-to-End Testing**: Automated testing script validates complete system functionality

### ‚úÖ **Robust Data Upload & Storage System** 
- **SECURE**: Multi-encoding support, file type validation, size limits, directory traversal protection
- **ATOMIC**: Database transactions with automatic cleanup on failure
- **VALIDATED**: Comprehensive CSV structure validation with flexible column matching
- **CACHED**: Intelligent word frequency caching with force-regeneration options
- **SCALABLE**: Pagination, efficient queries, background job tracking

### ‚úÖ **Production Infrastructure Stabilized** (RECENTLY FIXED ‚úÖ)
- **Frontend**: Vercel deployment with auto-deployment from GitHub
- **Backend**: Railway deployment FIXED with correct path configuration (`dockerfile` builder)  
- **Database**: PostgreSQL with enhanced connection pooling and health checks
- **CORS**: Fixed for cross-origin communication between Vercel and Railway
- **Configuration**: Railway.toml corrected after deployment path issues resolved
- **TypeScript**: WordCloudFilters interface updated with excludeWords and maxWords properties

### ‚úÖ **NEW: Flexible Data Ingestion System** (JUST IMPLEMENTED ‚úÖ)
- **JSON/CSV Support**: Upload any JSON or CSV file with automatic schema detection
- **AI-Powered Analysis**: Intelligent field type detection and role suggestions
- **Smart Field Mapping**: AI suggests which fields are primary text, categories, metadata, etc.
- **Custom Configuration**: User-defined analysis settings with live preview
- **Confidence Scoring**: Machine learning confidence scores for all field detections
- **Preview System**: See exactly what will be analyzed before processing

### ‚úÖ **Previous Achievements Maintained**
- NLTK Analysis Engine Enhanced (v2.5.0) with POS tagging
- Professional frontend with AG Grid dataset tables
- Word cloud system with multiple analysis modes
- Database persistence with comprehensive schema
- GitHub Actions deployment workflow
- Professional landing page and navigation

### ‚úÖ **Architecture Improvements**
- **No More Server Fragmentation**: Single source of truth for API endpoints
- **Proper Separation of Concerns**: API layer ‚Üí Service layer ‚Üí Database layer
- **Enhanced Error Handling**: Comprehensive exception handling with proper HTTP status codes
- **Development-Production Parity**: Same codebase for all environments
- **Maintainable Codebase**: Clear file organization and documentation

### üîß **Recent Deployment Fixes (September 2024)**
- **Railway Path Issues**: Fixed doubled path problem in container (`/app/backend/backend/` ‚Üí `/app/backend/`)
- **Build Configuration**: Corrected `railway.toml` to use `dockerfile` builder with proper PYTHONPATH
- **TypeScript Errors**: Added missing `excludeWords` and `maxWords` properties to `WordCloudFilters` interface
- **Health Checks**: Ensured `/health` endpoint responds correctly for Railway monitoring

### ‚ö†Ô∏è **Temporary Configurations**
- Authentication integration temporarily disabled for development
- File storage using local filesystem (AWS S3 integration ready)

### üéØ **Ready for Production**
The application now has a consolidated, robust architecture suitable for production deployment with enhanced data upload and storage capabilities.

## Next Steps
1. ‚úÖ ~~Set up database persistence (SQLite for development)~~ **COMPLETED**
2. ‚úÖ ~~Migrate existing datasets to database storage~~ **COMPLETED**  
3. ‚úÖ ~~Enhance frontend to use database API~~ **COMPLETED**
4. ‚úÖ ~~Frontend deployment to Vercel~~ **COMPLETED**
   - ‚úÖ GitHub repository setup and connected
   - ‚úÖ Vercel frontend deployment with auto-deployment
   - ‚úÖ Build errors resolved and configuration optimized
   - ‚úÖ Professional landing page and navigation restored
5. ‚úÖ ~~Complete backend deployment to Railway~~ **COMPLETED**
   - ‚úÖ Deploy FastAPI backend to Railway with PostgreSQL
   - ‚úÖ Configure environment variables for production
   - ‚úÖ Connect frontend to production backend API
   - ‚úÖ Fix CORS configuration for cross-origin requests
   - ‚úÖ Streamline GitHub Actions deployment workflow
6. ‚úÖ ~~Connect uploaded datasets to NLTK analysis pipeline~~ **COMPLETED**
   - ‚úÖ Full sentiment analysis (VADER + TextBlob)
   - ‚úÖ Entity extraction and topic analysis
   - ‚úÖ Enhanced word filtering with user exclusions
   - ‚úÖ **Part-of-Speech tagging for analysis mode differentiation**
   - ‚úÖ **NLTK-powered word categorization (verbs, nouns, adjectives, emotions)**
   - ‚úÖ **Force regeneration endpoints for testing enhanced filtering**
7. **üöÄ CURRENT: Test and refine NLTK analysis modes** to ensure distinct results between word cloud modes
8. **Implement real-time processing** with WebSocket updates and background jobs
9. **Build comprehensive analytics dashboard** components with database-powered insights
10. **Set up Celery workers** for background processing (async dataset processing)
11. **Add advanced analysis endpoints** (sentiment analysis API, entity extraction API, topic modeling)
12. **Enhance LLM integration** for business insights and response quality analysis
13. **Re-implement authentication** when ready for multi-user production

## **üéØ Immediate Priorities (December 2024)**
1. **Verify NLTK analysis modes are working** - Test that "all words" vs "action words" show different results
2. **Deploy NLTK enhancements** - Push version 2.5.0 with POS tagging to production
3. **User acceptance testing** - Validate the enhanced word filtering meets user requirements
4. **Performance optimization** - Ensure NLTK processing doesn't slow down word cloud generation

## **Working Application URLs**

### **üåê Production (LIVE) - UNIFIED ARCHITECTURE**
- **Frontend**: https://ai-text-analysis-platform.vercel.app (deployed on Vercel with auto-deployment)
- **Backend**: https://ai-text-analysis-production.up.railway.app (UNIFIED FastAPI server on Railway with PostgreSQL)
- **API Documentation**: https://ai-text-analysis-production.up.railway.app/docs (comprehensive API docs)
- **Health Check**: https://ai-text-analysis-production.up.railway.app/production/health (enhanced health monitoring)
- **System Info**: https://ai-text-analysis-production.up.railway.app/production/info (deployment details)

### **üè† Local Development**
- **Frontend**: http://localhost:3000 or http://localhost:3001 (Next.js application)
- **Dataset Upload**: http://localhost:3000/upload (CSV file upload interface)
- **Dashboard**: http://localhost:3000/dashboard (basic dashboard)
- **Word Cloud Demo**: http://localhost:3000/wordcloud-demo (interactive demo with backend data)
- **Legal Word Cloud**: http://localhost:3000/legal-wordcloud (filtered legal dataset visualization)

### **Backend API Server (Local Development):**
- **üÜï Unified API Server**: http://localhost:8000 (Consolidated FastAPI application)
- **API Documentation**: http://localhost:8000/docs (comprehensive API documentation)
- **Health Check**: http://localhost:8000/health (basic health status)
- **Production Health**: http://localhost:8000/production/health (detailed health information)
- **System Info**: http://localhost:8000/production/info (development environment details)

### **üß™ Testing & Validation:**
- **End-to-End Test**: `python backend/test_unified_system.py` (automated system validation)
- **Local Testing**: Tests unified server at http://localhost:8000
- **Production Testing**: Validates live deployment functionality

---

*This file should be updated by any AI working on this project to maintain current understanding of the system architecture and implementation status.*
